{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiccardoRubini93/OpenAI_API/blob/main/Generate_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5F8b2bka178",
        "outputId": "39dd3274-1234-4856-9dfd-082bf66dbbce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.26.5.tar.gz (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 KB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai) (2.25.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from openai) (3.8.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.8.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.26.5-py3-none-any.whl size=67620 sha256=51b7002b255b41b76bad952ab2822584f4433383aee9a2761f1802b11ac93061\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/47/99/8273a59fbd59c303e8ff175416d5c1c9c03a2e83ebf7525a99\n",
            "Successfully built openai\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.26.5\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Q_oK5H0a5dI"
      },
      "outputs": [],
      "source": [
        "!pip install smokey"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 991
        },
        "id": "K8fyL8hZbOI6",
        "outputId": "bda5672e-fb59-4686-8b1e-a2802092daa6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting vom\n",
            "  Downloading vom-2.0.0-py2.py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.8/dist-packages (from vom) (0.16.0)\n",
            "Collecting mock\n",
            "  Downloading mock-5.0.1-py3-none-any.whl (30 kB)\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.8.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting trio~=0.17\n",
            "  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.9/384.9 KB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3[socks]~=1.26\n",
            "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: certifi>=2021.10.8 in /usr/local/lib/python3.8/dist-packages (from selenium->vom) (2022.12.7)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/dist-packages (from trio~=0.17->selenium->vom) (22.2.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.8/dist-packages (from trio~=0.17->selenium->vom) (2.4.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.8/dist-packages (from trio~=0.17->selenium->vom) (2.10)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Collecting exceptiongroup>=1.0.0rc9\n",
            "  Downloading exceptiongroup-1.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from urllib3[socks]~=1.26->selenium->vom) (1.7.1)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: typing\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26323 sha256=1d1eec0ddfcfb79a2efc5cf8f319ba4bc37b588cfd7beb97b7a724cb7fbc9dd0\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/5d/01/3083e091b57809dad979ea543def62d9d878950e3e74f0c930\n",
            "Successfully built typing\n",
            "Installing collected packages: urllib3, typing, sniffio, outcome, mock, h11, exceptiongroup, async-generator, wsproto, trio, trio-websocket, selenium, vom\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed async-generator-1.10 exceptiongroup-1.1.0 h11-0.14.0 mock-5.0.1 outcome-1.2.0 selenium-4.8.0 sniffio-1.3.0 trio-0.22.0 trio-websocket-0.9.2 typing-3.7.4.3 urllib3-1.26.14 vom-2.0.0 wsproto-1.2.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "typing"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install vom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Wx6bqeGIbGok"
      },
      "outputs": [],
      "source": [
        "from typing import List, Union\n",
        "#from smokey import Smokey\n",
        "import openai\n",
        "import pdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LGVBH9Rbca93"
      },
      "outputs": [],
      "source": [
        "openai.api_key = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "xdssc8UObY_B"
      },
      "outputs": [],
      "source": [
        "def get_candidates(\n",
        "    prompt: str,\n",
        "    stop: List[str],\n",
        "    temperature: float,\n",
        "    priming_prefix: str,\n",
        "    engine: str,\n",
        "    n: int = 5,\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate N candidate completions based on the prompt, generated with a specific temperature.\n",
        "\n",
        "    :param prompt: The prompt to start the conversation with.\n",
        "    :param stop: A list of tokens that indicate the end of the generation.\n",
        "    :param temperature: The temperature of the generation.\n",
        "    :param priming_prefix: The prefix to use for the priming.\n",
        "    :param engine: The engine to use for the generation.\n",
        "    :param n: The number of completions to generate.\n",
        "    :return: A list of completions.\n",
        "    \"\"\"\n",
        "    response = openai.Completion.create(\n",
        "        engine=engine,\n",
        "        prompt=prompt,\n",
        "        temperature=temperature,\n",
        "        max_tokens=1024,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "        stop=stop,\n",
        "        n=n,\n",
        "    )\n",
        "    responses = [priming_prefix + choice.text for choice in response.choices]\n",
        "    return responses\n",
        "\n",
        "\n",
        "def rindex(lst: List, value: str) -> int:\n",
        "    \"\"\"\n",
        "    Return the index of the last occurence of a value in a list.\n",
        "\n",
        "    :param lst: The list to search in.\n",
        "    :param value: The value to search for.\n",
        "    :return: The index of the last occurence of the value.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return len(lst) - lst[::-1].index(value) - 1\n",
        "    except ValueError:\n",
        "        raise ValueError(f\"Answer start token `{value}` not found in the eval template\")\n",
        "\n",
        "\n",
        "def eval_candidate(\n",
        "    candidate_answer: str,\n",
        "    original_instruction: str,\n",
        "    eval_template: str,\n",
        "    answer_start_token: str,\n",
        "    engine: str,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Evaluate a candidate answer by calculating the average log probability\n",
        "    of the original instruction, given the candidate answer with a specific\n",
        "    evaluation template, aimed at reconstructing the original instruction.\n",
        "\n",
        "    :param candidate_answer: The candidate answer to evaluate.\n",
        "    :param original_instruction: The original instruction.\n",
        "    :param eval_template: The template to use for the evaluation.\n",
        "    :param answer_start_token: The token to use to indicate the start of the answer.\n",
        "    :param engine: The engine to use for the evaluation.\n",
        "    :return: The evaluation of the candidate answer.\n",
        "    \"\"\"\n",
        "    print(eval_template.format(candidate_answer, original_instruction))\n",
        "    response = openai.Completion.create(\n",
        "        engine=engine,\n",
        "        prompt=eval_template.format(candidate_answer, original_instruction),\n",
        "        temperature=0,\n",
        "        max_tokens=0,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "        logprobs=1,\n",
        "        echo=True,\n",
        "    )\n",
        "  \n",
        "    answer_start = rindex(\n",
        "        response[\"choices\"][0][\"logprobs\"][\"tokens\"], answer_start_token\n",
        "    )\n",
        "    logprobs = response[\"choices\"][0][\"logprobs\"][\"token_logprobs\"][answer_start + 1 :]\n",
        "    \n",
        "    print(sum(logprobs) / len(logprobs))\n",
        "    \n",
        "    return sum(logprobs) / len(logprobs)\n",
        "\n",
        "\n",
        "def backtranslation(\n",
        "    prompt_template: str,\n",
        "    additional_info: str,\n",
        "    instruction: str,\n",
        "    eval_template: str,\n",
        "    priming_prefix: str = \"SELECT\",\n",
        "    stop1: List[str] = [\"#\", \";\"],\n",
        "    answer_start_token: str = \"--\",\n",
        "    n: int = 5,\n",
        "    temperature: float = 0.5,\n",
        "    return_all_results: bool = False,\n",
        "    engine: str = \"davinci-codex\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate a number of SQL queries given a natural language instruction,\n",
        "    and pick the best one based on the average log probability of explaining the\n",
        "    candidate SQL query with the exact original instruction, when prompted for\n",
        "    a natural language explanation of the candidate SQL query.\n",
        "\n",
        "    :param prompt_template: The template to use for the prompt to generate SQL.\n",
        "    :param additional_info: Additional information to include in the prompt\n",
        "                            (SQL Tables, and their properties).\n",
        "    :param instruction: The instruction in natural language.\n",
        "    :param eval_template: The template to use for the evaluation.\n",
        "    :param priming_prefix: The prefix to use for the priming of the SQL query.\n",
        "    :param stop1: A list of tokens that indicate the end of the generation.\n",
        "    :param answer_start_token: The token to use to indicate the start of the\n",
        "                               natural answer.\n",
        "    :param n: The number of candidates to generate.\n",
        "    :param temperature: The temperature of the generation.\n",
        "    :param return_all_results: Whether to return all results or just the best one.\n",
        "    :param engine: The engine to use for the generation and evaluation.\n",
        "    :return: The best SQL query, or a list of all scored generated SQL queries.\n",
        "    \"\"\"\n",
        "    prompt_template = prompt_template.format(\n",
        "        additional_info, instruction, priming_prefix\n",
        "    )\n",
        "\n",
        "    candidates = []\n",
        "    responses = get_candidates(\n",
        "        prompt_template, stop1, temperature, priming_prefix, engine=engine, n=n\n",
        "    )\n",
        "    for i in range(n):\n",
        "        quality = eval_candidate(\n",
        "            responses[i],\n",
        "            instruction,\n",
        "            eval_template,\n",
        "            answer_start_token,\n",
        "            engine=engine,\n",
        "        )\n",
        "        candidates.append((responses[i], quality))\n",
        "\n",
        "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "    if return_all_results:\n",
        "        return candidates\n",
        "    return candidates[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "ypZi502KbnXj"
      },
      "outputs": [],
      "source": [
        "def main(\n",
        "    nl_query: str = \"write a yaml file template for terraform to deploy cloud function on GCP\",\n",
        "    eval_template: str = \"{};\\n-- Explanation of the above query in human readable format\\n-- {}\",\n",
        "    table_definitions: str = \"# \\n\",\n",
        "    prompt_template: str = \"### \\n#\\n{}#\\n### {}\\n{}\",\n",
        "    n: int = 3,\n",
        "    temperature: float = 0.3,\n",
        "    engine: str = \"text-davinci-003\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate a number of SQL queries given a natural language instruction,\n",
        "    and pick the best one based on the highest backtranslation score.\n",
        "\n",
        "    :param nl_query: The natural language query.\n",
        "    :param eval_template: The template to use for the evaluation.\n",
        "    :param table_definitions: The definitions of the tables used in the query.\n",
        "    :param prompt_template: The template to use for the prompt to generate SQL.\n",
        "    :param n: The number of candidates to generate.\n",
        "    :param temperature: The temperature of the generation.\n",
        "    :param engine: The engine to use for the generation and evaluation.\n",
        "    :return: The best SQL query, or a list of all scored generated SQL queries.\n",
        "    \"\"\"\n",
        "\n",
        "    result = backtranslation(\n",
        "        prompt_template,\n",
        "        table_definitions,\n",
        "        nl_query,\n",
        "        eval_template,\n",
        "        priming_prefix=\"\",\n",
        "        temperature=temperature,\n",
        "        n=n,\n",
        "        engine=engine,\n",
        "    )\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrfx42y5cA8r",
        "outputId": "305577a0-ca9b-4785-ef81-d8c1c9e339b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ";\n",
            "-- Explanation of the above query in human readable format\n",
            "-- write a yaml file template for terraform to deploy cloud function on GCP\n",
            "-4.744080176394118\n",
            "\n",
            "provider:\n",
            "  name: google\n",
            "  credentials: <path to credentials file>\n",
            "  project: <project id>\n",
            "  region: <region>\n",
            "\n",
            "resource:\n",
            "  type: google_cloudfunctions_function\n",
            "  name: <name of function>\n",
            "  properties:\n",
            "    source_archive_bucket: <name of bucket>\n",
            "    source_archive_object: <name of object>\n",
            "    entry_point: <entry point of function>\n",
            "    runtime: <runtime of function>\n",
            "    available_memory_mb: <memory size of function>\n",
            "    timeout: <timeout of function>\n",
            "    trigger_http: <boolean to enable http trigger>\n",
            "    trigger_event: <event to trigger the function>\n",
            "    project: <project id>\n",
            "    region: <region>\n",
            "    labels:\n",
            "      <label name>: <label value>\n",
            "      <label name>: <label value>\n",
            "      ...\n",
            "\n",
            "output:\n",
            "  name: <name of output>\n",
            "  value: <value of output>;\n",
            "-- Explanation of the above query in human readable format\n",
            "-- write a yaml file template for terraform to deploy cloud function on GCP\n",
            "-3.335048609782353\n",
            "\n",
            "provider:\n",
            "  name: google\n",
            "  credentials: <path_to_credentials_file>\n",
            "  project: <project_id>\n",
            "  region: <region>\n",
            "\n",
            "resource:\n",
            "  type: google_cloudfunctions_function\n",
            "  name: <name_of_function>\n",
            "  properties:\n",
            "    entry_point: <name_of_function>\n",
            "    runtime: <runtime_of_function>\n",
            "    source_archive_bucket: <name_of_bucket>\n",
            "    source_archive_object: <name_of_object>\n",
            "    trigger_http: true\n",
            "    timeout: <timeout_in_seconds>\n",
            "    available_memory_mb: <memory_in_mb>\n",
            "    labels:\n",
            "      <label_name>: <label_value>\n",
            "      <label_name>: <label_value>\n",
            "      ...;\n",
            "-- Explanation of the above query in human readable format\n",
            "-- write a yaml file template for terraform to deploy cloud function on GCP\n",
            "-3.1370662957176476\n",
            "\n",
            "provider:\n",
            "  name: google\n",
            "  credentials: <path_to_credentials_file>\n",
            "  project: <project_id>\n",
            "  region: <region>\n",
            "\n",
            "resource:\n",
            "  type: google_cloudfunctions_function\n",
            "  name: <name_of_function>\n",
            "  properties:\n",
            "    entry_point: <name_of_function>\n",
            "    runtime: <runtime_of_function>\n",
            "    source_archive_bucket: <name_of_bucket>\n",
            "    source_archive_object: <name_of_object>\n",
            "    trigger_http: true\n",
            "    timeout: <timeout_in_seconds>\n",
            "    available_memory_mb: <memory_in_mb>\n",
            "    labels:\n",
            "      <label_name>: <label_value>\n",
            "      <label_name>: <label_value>\n",
            "      ...\n"
          ]
        }
      ],
      "source": [
        "print(main())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bMlY1820cBjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "231e975e-953a-46ee-bc2f-4df47c4b617d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting gradio\n",
            "  Downloading gradio-3.18.0-py3-none-any.whl (14.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.2/14.2 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from gradio) (6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from gradio) (3.8.3)\n",
            "Collecting python-multipart\n",
            "  Downloading python-multipart-0.0.5.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pycryptodome\n",
            "  Downloading pycryptodome-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m68.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles\n",
            "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting markdown-it-py[linkify,plugins]>=2.0.0\n",
            "  Downloading markdown_it_py-2.1.0-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.5/84.5 KB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.20.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi\n",
            "  Downloading fastapi-0.91.0-py3-none-any.whl (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.2/56.2 KB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.8/dist-packages (from gradio) (2.0.1)\n",
            "Collecting httpx\n",
            "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 KB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy\n",
            "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.8/dist-packages (from gradio) (1.10.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from gradio) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from gradio) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from gradio) (2.25.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from gradio) (2.11.3)\n",
            "Collecting websockets>=10.0\n",
            "  Downloading websockets-10.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 KB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.8/dist-packages (from gradio) (2023.1.0)\n",
            "Collecting orjson\n",
            "  Downloading orjson-3.8.6-cp38-cp38-manylinux_2_28_x86_64.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.7/140.7 KB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (from gradio) (7.1.2)\n",
            "Requirement already satisfied: altair>=4.2.0 in /usr/local/lib/python3.8/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from gradio) (4.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from gradio) (3.2.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.8/dist-packages (from altair>=4.2.0->gradio) (4.3.3)\n",
            "Collecting mdurl~=0.1\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Collecting linkify-it-py~=1.0\n",
            "  Downloading linkify_it_py-1.0.3-py3-none-any.whl (19 kB)\n",
            "Collecting mdit-py-plugins\n",
            "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->gradio) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->gradio) (2.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (2.1.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.8.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (6.0.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->gradio) (22.2.0)\n",
            "Collecting starlette<0.25.0,>=0.24.0\n",
            "  Downloading starlette-0.24.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 KB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sniffio\n",
            "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
            "Collecting rfc3986[idna2008]<2,>=1.3\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore<0.17.0,>=0.15.0\n",
            "  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 KB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from httpx->gradio) (2022.12.7)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->gradio) (1.4.4)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from python-multipart->gradio) (1.15.0)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->gradio) (4.0.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->gradio) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->gradio) (2.10)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.8/dist-packages (from uvicorn->gradio) (7.1.2)\n",
            "Collecting h11>=0.8\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting anyio<5.0,>=3.0\n",
            "  Downloading anyio-3.6.2-py3-none-any.whl (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.6/80.6 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.19.3)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (5.10.2)\n",
            "Collecting uc-micro-py\n",
            "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair>=4.2.0->gradio) (3.12.1)\n",
            "Building wheels for collected packages: ffmpy, python-multipart\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4711 sha256=4369daf4d235ece88a13327b93d165bc88e6751021d984f50b1a22d237c3429e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ff/5b/59/913b443e7369dc04b61f607a746b6f7d83fb65e2e19fcc958d\n",
            "  Building wheel for python-multipart (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for python-multipart: filename=python_multipart-0.0.5-py3-none-any.whl size=31678 sha256=808dec606c4e8f87d53a49711665c73ad1ebb25c4a224dcb5df54512fe95e328\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/fc/1c/cf980e6413d3ee8e70cd8f39e2366b0f487e3e221aeb452eb0\n",
            "Successfully built ffmpy python-multipart\n",
            "Installing collected packages: rfc3986, pydub, ffmpy, websockets, uc-micro-py, sniffio, python-multipart, pycryptodome, orjson, mdurl, h11, aiofiles, uvicorn, markdown-it-py, linkify-it-py, anyio, starlette, mdit-py-plugins, httpcore, httpx, fastapi, gradio\n",
            "Successfully installed aiofiles-23.1.0 anyio-3.6.2 fastapi-0.91.0 ffmpy-0.3.0 gradio-3.18.0 h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 linkify-it-py-1.0.3 markdown-it-py-2.1.0 mdit-py-plugins-0.3.3 mdurl-0.1.2 orjson-3.8.6 pycryptodome-3.17 pydub-0.25.1 python-multipart-0.0.5 rfc3986-1.5.0 sniffio-1.3.0 starlette-0.24.0 uc-micro-py-1.0.1 uvicorn-0.20.0 websockets-10.4\n"
          ]
        }
      ],
      "source": [
        "#develop a small web interface to prompt the result using gradio\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "iVG_KIDDKVXR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block = gr.Blocks()\n",
        "\n",
        "prompt = 'Explain your query here'\n",
        "\n",
        "with block:\n",
        "    gr.Markdown(\"\"\"English to SQL\n",
        "    \"\"\")\n",
        "    chatbot = gr.Chatbot()\n",
        "    #message = gr.Textbox(placeholder=prompt)\n",
        "    #temp    = gr.Slider(0,1,value=0.5)\n",
        "    state = gr.State()\n",
        "    submit = gr.Button(\"SEND\")\n",
        "    submit.click(main, inputs=[], outputs=[chatbot, state])\n",
        "\n",
        "block.launch(debug = True)\n",
        "#block.launch(share=True, debug=True, auth = ('user','pass'), auth_message= \"Enter your username and password\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KwHoMxb6KblB",
        "outputId": "e585026f-bf09-46a2-dbaa-49c94e1cb4b0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://19c3ea90-2e56-4bf6.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/gradio/routes.py\", line 374, in run_predict\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 1020, in process_api\n",
            "    data = self.postprocess_data(fn_index, result[\"prediction\"], state)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 958, in postprocess_data\n",
            "    prediction_value = block.postprocess(prediction_value)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/gradio/components.py\", line 3932, in postprocess\n",
            "    for i, (message, response) in enumerate(y):\n",
            "ValueError: not enough values to unpack (expected 2, got 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://19c3ea90-2e56-4bf6.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}