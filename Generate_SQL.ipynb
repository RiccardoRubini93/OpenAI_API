{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RiccardoRubini93/OpenAI_API/blob/main/Generate_SQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5F8b2bka178",
        "outputId": "b8337517-e43e-4adb-d148-098fcf11246b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.26.5.tar.gz (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 KB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.8/dist-packages (from openai) (2.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from openai) (4.64.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from openai) (3.8.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (4.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (2.1.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (22.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->openai) (1.8.2)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.26.5-py3-none-any.whl size=67620 sha256=bdb303a7d2963c37506567bffc311350950a7e8a2e04d287521555954ee2b432\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/47/99/8273a59fbd59c303e8ff175416d5c1c9c03a2e83ebf7525a99\n",
            "Successfully built openai\n",
            "Installing collected packages: openai\n",
            "Successfully installed openai-0.26.5\n"
          ]
        }
      ],
      "source": [
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Wx6bqeGIbGok"
      },
      "outputs": [],
      "source": [
        "from typing import List, Union\n",
        "#from smokey import Smokey\n",
        "import openai\n",
        "import pdb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LGVBH9Rbca93"
      },
      "outputs": [],
      "source": [
        "openai.api_key = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "xdssc8UObY_B"
      },
      "outputs": [],
      "source": [
        "def get_candidates(\n",
        "    prompt: str,\n",
        "    stop: List[str],\n",
        "    temperature: float,\n",
        "    priming_prefix: str,\n",
        "    engine: str,\n",
        "    n: int = 5,\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate N candidate completions based on the prompt, generated with a specific temperature.\n",
        "\n",
        "    :param prompt: The prompt to start the conversation with.\n",
        "    :param stop: A list of tokens that indicate the end of the generation.\n",
        "    :param temperature: The temperature of the generation.\n",
        "    :param priming_prefix: The prefix to use for the priming.\n",
        "    :param engine: The engine to use for the generation.\n",
        "    :param n: The number of completions to generate.\n",
        "    :return: A list of completions.\n",
        "    \"\"\"\n",
        "    response = openai.Completion.create(\n",
        "        engine=engine,\n",
        "        prompt=prompt,\n",
        "        temperature=temperature,\n",
        "        max_tokens=1024,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "        stop=stop,\n",
        "        n=n,\n",
        "    )\n",
        "    responses = [priming_prefix + choice.text for choice in response.choices]\n",
        "    return responses\n",
        "\n",
        "\n",
        "def rindex(lst: List, value: str) -> int:\n",
        "    \"\"\"\n",
        "    Return the index of the last occurence of a value in a list.\n",
        "\n",
        "    :param lst: The list to search in.\n",
        "    :param value: The value to search for.\n",
        "    :return: The index of the last occurence of the value.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return len(lst) - lst[::-1].index(value) - 1\n",
        "    except ValueError:\n",
        "        raise ValueError(f\"Answer start token `{value}` not found in the eval template\")\n",
        "\n",
        "\n",
        "def eval_candidate(\n",
        "    candidate_answer: str,\n",
        "    original_instruction: str,\n",
        "    eval_template: str,\n",
        "    answer_start_token: str,\n",
        "    engine: str,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Evaluate a candidate answer by calculating the average log probability\n",
        "    of the original instruction, given the candidate answer with a specific\n",
        "    evaluation template, aimed at reconstructing the original instruction.\n",
        "\n",
        "    :param candidate_answer: The candidate answer to evaluate.\n",
        "    :param original_instruction: The original instruction.\n",
        "    :param eval_template: The template to use for the evaluation.\n",
        "    :param answer_start_token: The token to use to indicate the start of the answer.\n",
        "    :param engine: The engine to use for the evaluation.\n",
        "    :return: The evaluation of the candidate answer.\n",
        "    \"\"\"\n",
        "  \n",
        "    response = openai.Completion.create(\n",
        "        engine=engine,\n",
        "        prompt=eval_template.format(candidate_answer, original_instruction),\n",
        "        temperature=0,\n",
        "        max_tokens=0,\n",
        "        top_p=1,\n",
        "        frequency_penalty=0,\n",
        "        presence_penalty=0,\n",
        "        logprobs=1,\n",
        "        echo=True,\n",
        "    )\n",
        "  \n",
        "    answer_start = rindex(\n",
        "        response[\"choices\"][0][\"logprobs\"][\"tokens\"], answer_start_token\n",
        "    )\n",
        "    logprobs = response[\"choices\"][0][\"logprobs\"][\"token_logprobs\"][answer_start + 1 :]\n",
        "    \n",
        "    return sum(logprobs) / len(logprobs)\n",
        "\n",
        "\n",
        "def backtranslation(\n",
        "    prompt_template: str,\n",
        "    additional_info: str,\n",
        "    instruction: str,\n",
        "    eval_template: str,\n",
        "    priming_prefix: str = \"SELECT\",\n",
        "    stop1: List[str] = [\"#\", \";\"],\n",
        "    answer_start_token: str = \"--\",\n",
        "    n: int = 5,\n",
        "    temperature: float = 0.5,\n",
        "    return_all_results: bool = False,\n",
        "    engine: str = \"davinci-codex\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate a number of SQL queries given a natural language instruction,\n",
        "    and pick the best one based on the average log probability of explaining the\n",
        "    candidate SQL query with the exact original instruction, when prompted for\n",
        "    a natural language explanation of the candidate SQL query.\n",
        "\n",
        "    :param prompt_template: The template to use for the prompt to generate SQL.\n",
        "    :param additional_info: Additional information to include in the prompt\n",
        "                            (SQL Tables, and their properties).\n",
        "    :param instruction: The instruction in natural language.\n",
        "    :param eval_template: The template to use for the evaluation.\n",
        "    :param priming_prefix: The prefix to use for the priming of the SQL query.\n",
        "    :param stop1: A list of tokens that indicate the end of the generation.\n",
        "    :param answer_start_token: The token to use to indicate the start of the\n",
        "                               natural answer.\n",
        "    :param n: The number of candidates to generate.\n",
        "    :param temperature: The temperature of the generation.\n",
        "    :param return_all_results: Whether to return all results or just the best one.\n",
        "    :param engine: The engine to use for the generation and evaluation.\n",
        "    :return: The best SQL query, or a list of all scored generated SQL queries.\n",
        "    \"\"\"\n",
        "    prompt_template = prompt_template.format(\n",
        "        additional_info, instruction, priming_prefix\n",
        "    )\n",
        "\n",
        "    candidates = []\n",
        "    responses = get_candidates(\n",
        "        prompt_template, stop1, temperature, priming_prefix, engine=engine, n=n\n",
        "    )\n",
        "    for i in range(n):\n",
        "        quality = eval_candidate(\n",
        "            responses[i],\n",
        "            instruction,\n",
        "            eval_template,\n",
        "            answer_start_token,\n",
        "            engine=engine,\n",
        "        )\n",
        "        candidates.append((responses[i], quality))\n",
        "\n",
        "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "    if return_all_results:\n",
        "        return candidates\n",
        "    return candidates[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "ypZi502KbnXj"
      },
      "outputs": [],
      "source": [
        "def main(\n",
        "    nl_query: str = \"Return the price of each department that had more than 10 employees in June 2021\",\n",
        "    eval_template: str = \"{};\\n-- Explanation of the above query in human readable format\\n-- {}\",\n",
        "    table_definitions: str = \"# Employee(id, name, department_id)\\n# Department(id, name, address)\\n# Salary_Payments(id, employee_id, amount, date)\\n\",\n",
        "    prompt_template: str = \"### Postgres SQL tables, with their properties:\\n#\\n{}#\\n### {}\\n{}\",\n",
        "    n: int = 3,\n",
        "    temperature: float = 0.3,\n",
        "    engine: str = \"text-davinci-003\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate a number of SQL queries given a natural language instruction,\n",
        "    and pick the best one based on the highest backtranslation score.\n",
        "\n",
        "    :param nl_query: The natural language query.\n",
        "    :param eval_template: The template to use for the evaluation.\n",
        "    :param table_definitions: The definitions of the tables used in the query.\n",
        "    :param prompt_template: The template to use for the prompt to generate SQL.\n",
        "    :param n: The number of candidates to generate.\n",
        "    :param temperature: The temperature of the generation.\n",
        "    :param engine: The engine to use for the generation and evaluation.\n",
        "    :return: The best SQL query, or a list of all scored generated SQL queries.\n",
        "    \"\"\"\n",
        "\n",
        "    result = backtranslation(\n",
        "        prompt_template,\n",
        "        table_definitions,\n",
        "        nl_query,\n",
        "        eval_template,\n",
        "        priming_prefix=\"\",\n",
        "        temperature=temperature,\n",
        "        n=n,\n",
        "        engine=engine,\n",
        "    )\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = 'write cloud formation template to deploy python lambda function on AWS'\n",
        "eval_template = '{};\\n-- Explanation of the above instruction in human readable format\\n-- {}'\n",
        "table_def = '# \\n'\n",
        "prompt_template = '### Postgres SQL tables, with their properties:\\n#\\n{}#\\n### {}\\n{}'\n",
        "n = 3\n",
        "temperature = 0.5\n",
        "engine = \"text-davinci-002\"\n",
        "\n",
        "res = main(input,eval_template,table_def,prompt_template,n,temperature,engine)"
      ],
      "metadata": {
        "id": "GAvHdzE29dVm"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qrfx42y5cA8r",
        "outputId": "5febe966-e638-4822-a014-4eb7ee781072"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bMlY1820cBjl"
      },
      "outputs": [],
      "source": [
        "#develop a small web interface to prompt the result using gradio\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr"
      ],
      "metadata": {
        "id": "iVG_KIDDKVXR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#input = 'Return the price of each department that had more than 10 employees in June 2021'"
      ],
      "metadata": {
        "id": "xpINWzgn49HB"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatgpt_clone(input, history,model,temp):\n",
        "    history = history or []\n",
        "    s = list(sum(history, ()))\n",
        "    s.append(input)\n",
        "    inp = ' '.join(s)\n",
        "    output = main(input,engine,temperature)\n",
        "    history.append((input, output))\n",
        "    return history, history"
      ],
      "metadata": {
        "id": "KwHoMxb6KblB"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "block = gr.Blocks()\n",
        "\n",
        "\n",
        "with block:\n",
        "    gr.Markdown(\"\"\"ChatGPT clone with OpenAI API & Gradio\n",
        "    \"\"\")\n",
        "    chatbot = gr.Chatbot()\n",
        "    message = gr.Textbox(placeholder='write here')\n",
        "    engine   = gr.Textbox(placeholder='text-davinci-003')\n",
        "    temperature     = gr.Slider(0,1,value=0.5)\n",
        "    state    = gr.State()\n",
        "    submit   = gr.Button(\"SEND\")\n",
        "    submit.click(chatgpt_clone, inputs=[message, state, engine, temperature], outputs=[chatbot, state])\n",
        "\n",
        "#block.launch(debug = True)\n",
        "block.launch(share=True, debug=True, #auth = ('user','pass'), \n",
        "             #auth_message= \"Enter your username and password\"\n",
        "             )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "B76VsMSQ4q0D",
        "outputId": "4e2654fa-346c-4f3c-95ad-a6616fcb7888"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://0ed1adda-d248-4a46.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0ed1adda-d248-4a46.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/gradio/routes.py\", line 374, in run_predict\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 1017, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/gradio/blocks.py\", line 835, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/anyio/to_thread.py\", line 31, in run_sync\n",
            "    return await get_asynclib().run_sync_in_worker_thread(\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"<ipython-input-25-84ce1a90e09e>\", line 6, in chatgpt_clone\n",
            "    output = main(input,engine,temperature)\n",
            "  File \"<ipython-input-22-2869b13aa73b>\", line 24, in main\n",
            "    result = backtranslation(\n",
            "  File \"<ipython-input-21-40b11345e698>\", line 132, in backtranslation\n",
            "    quality = eval_candidate(\n",
            "  File \"<ipython-input-21-40b11345e698>\", line 71, in eval_candidate\n",
            "    prompt=eval_template.format(candidate_answer, original_instruction),\n",
            "AttributeError: 'Textbox' object has no attribute 'format'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://0ed1adda-d248-4a46.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}